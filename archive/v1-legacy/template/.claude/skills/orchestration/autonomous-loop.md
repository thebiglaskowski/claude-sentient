---
name: autonomous-loop
description: Continuously cycle through quality gates until all issues resolved
argument-hint: "[focus area] [--max-iterations=N]"
model: opus
version: 3.2.0
tags: [orchestration, workflow, autonomous]
---

# Autonomous Development Loop v3.2

Continuously cycle through 10 phases and 15 quality gates until every issue is resolved and every checklist passes. Integrates with hooks for automated context injection, error recovery, and agent coordination.

**v3.2 Enhancements:**
- Context budget monitoring (prevents context bloat)
- Parallel task decomposition (faster execution)
- Decision logging (traceability)
- Commit checkpoints (easy rollback)

## Description

This skill implements an exhaustive development loop with **STRICT ENFORCEMENT**:
- Runs all 12 quality gates (security, testing, code quality, etc.)
- Identifies and fixes issues automatically
- Uses hooks for context injection and error recovery
- Coordinates parallel agents for comprehensive analysis
- **CANNOT EXIT until ALL thresholds are met**
- Verifies Definition of Done with actual test/coverage runs
- No warnings, no exceptions â€” every gate must pass

**STRICT MODE**: The loop will continue indefinitely until:
- Test coverage >= 80%
- 0 lint errors, 0 type errors
- 0 security vulnerabilities (S0/S1)
- All tests passing
- Documentation complete

Triggers on: "autonomous loop", "keep working", "work until done", "exhaustive mode", "/loop"

## Arguments

```
$1 - Optional: focus area (security, testing, features, all)
     Default: all

--max-iterations=N  Maximum loop iterations (default: 50)
--pause-on=S0|S1    Pause for user on severity level (default: S0 only)
--modern            Force modern tech alternatives check
--parallel          Enable parallel agent execution
--dry-run           Show what would be done without doing it
```

## STRICT MODE THRESHOLDS (Non-Negotiable)

The loop CANNOT exit until ALL of these are met:

### Code Quality Gates

| Metric | Threshold | Verified By |
|--------|-----------|-------------|
| Test Coverage (overall) | >= 80% | Running tests with coverage |
| Test Coverage (new code) | >= 90% | Running tests with coverage |
| Lint Errors | 0 | Running linter |
| Lint Warnings | 0 | Running linter |
| Type Errors | 0 | Running type checker |
| Security S0 | 0 | Running security audit |
| Security S1 | 0 | Running security audit |
| Failing Tests | 0 | Running test suite |

### Work Completion Gates

| Metric | Threshold | Verified By |
|--------|-----------|-------------|
| Work Queue | Empty | Parsing LOOP_STATE.md, STATUS.md |
| Pending Tasks | 0 | Counting â³/[ ] markers |
| In-Progress Tasks | 0 | Counting ğŸ”„/[-] markers |
| Blocked Tasks | 0 | Counting blocked items |
| S0 Issues | 0 unresolved | Parsing KNOWN_ISSUES.md |
| S1 Issues | 0 unresolved | Parsing KNOWN_ISSUES.md |
| README.md | Exists | File check |
| CHANGELOG.md | Updated | Content check |
| Git State | Clean | Running git status |

**These thresholds are enforced by `dod-verifier.py` (Stop hook).**

The hook checks EVERYTHING:
- Runs test/lint/type/security commands and parses output
- Parses LOOP_STATE.md for pending work
- Parses KNOWN_ISSUES.md for unresolved S0/S1
- Checks git status for uncommitted changes
- Verifies documentation exists

**Exit code 1 = loop continues (work incomplete)**
**Exit code 0 = loop may exit (ALL gates pass)**

## The Loop (10 Phases - With Intelligence)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               INTELLIGENT AUTONOMOUS LOOP v3.2 (10 Phases)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚   â”‚ CONTEXTUALIZE â”‚â”€â”€â”€â–¶â”‚ ASSESS  â”‚â”€â”€â”€â–¶â”‚ META-COGNITIONâ”‚                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚          â–²                                     â”‚                         â”‚
â”‚          â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚          â”‚              â–¼                                                â”‚
â”‚          â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚          â”‚        â”‚  PLAN   â”‚â”€â”€â”€â–¶â”‚  BUILD  â”‚â”€â”€â”€â–¶â”‚  TEST   â”‚            â”‚
â”‚          â”‚        â”‚+DECOMP  â”‚    â”‚+DECIDE  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚          â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚                  â”‚
â”‚          â”‚                                           â–¼                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚  DONE   â”‚â—€â”€â”€â”€â”‚ EVALUATE â”‚â—€â”€â”€â”€â”‚CHECKPOINTâ”‚â—€â”€â”€â”‚ QUALITY â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜           â”‚
â”‚          â–²              â”‚              â”‚              â”‚                  â”‚
â”‚          â”‚         ALL PASS?      GIT COMMIT    REASSESS?               â”‚
â”‚          â”‚              â”‚              â”‚              â”‚                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      YES  â”‚              â”‚        YES   â”‚                 â”‚
â”‚   â”‚ RECOVER â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                                           â”‚
â”‚        â”‚ Smart recovery using capability inventory                      â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Back to CONTEXTUALIZE with new strategy         â”‚
â”‚                                                                          â”‚
â”‚   ENHANCED PHASES:                                                       â”‚
â”‚   â€¢ PLAN+DECOMP: Parallel task decomposition for efficiency             â”‚
â”‚   â€¢ BUILD+DECIDE: Decision logging for traceability                     â”‚
â”‚   â€¢ CHECKPOINT: Git commits after verified features                     â”‚
â”‚   â€¢ META-COGNITION: Context budget monitoring, approach selection       â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quality Gates (All Blocking)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              QUALITY GATES - ALL BLOCKING (NO EXCEPTIONS)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  CODE QUALITY GATES:                                                     â”‚
â”‚  GATE 1:  PRE-FLIGHT      Environment, dependencies, git state         â”‚
â”‚  GATE 2:  LINT            0 errors, 0 warnings                         â”‚
â”‚  GATE 3:  TYPE            0 type errors                                â”‚
â”‚  GATE 4:  UNIT TEST       100% pass, coverage >= 80%                   â”‚
â”‚  GATE 5:  INTEGRATION     100% pass                                    â”‚
â”‚  GATE 6:  SECURITY        0 S0/S1 vulnerabilities                      â”‚
â”‚  GATE 7:  PERFORMANCE     Bundle size, N+1, Core Web Vitals           â”‚
â”‚  GATE 8:  BROWSER         Visual verification, responsive             â”‚
â”‚  GATE 9:  ACCESSIBILITY   axe-core, keyboard nav                      â”‚
â”‚  GATE 10: DOCUMENTATION   README + CHANGELOG present                   â”‚
â”‚  GATE 11: MODERN TECH     No deprecated APIs                          â”‚
â”‚                                                                          â”‚
â”‚  WORK COMPLETION GATES:                                                  â”‚
â”‚  GATE 12: WORK QUEUE      0 pending/in-progress tasks                  â”‚
â”‚  GATE 13: KNOWN ISSUES    0 S0/S1 unresolved                          â”‚
â”‚  GATE 14: GIT STATE       All changes committed                        â”‚
â”‚  GATE 15: DEFINITION OF DONE  dod-verifier.py exit code 0             â”‚
â”‚                                                                          â”‚
â”‚  Loop CANNOT exit until ALL gates pass. No warnings. No exceptions.    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Hook Integration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HOOK INTEGRATION                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  SessionStart       â†’ session-start.sh      Initialize session state    â”‚
â”‚  UserPromptSubmit   â†’ context-injector.py   Load relevant context       â”‚
â”‚  PreToolUse         â†’ bash-auto-approve.py  Auto-approve safe commands  â”‚
â”‚  PreToolUse         â†’ file-validator.py     Validate file operations    â”‚
â”‚  PostToolUse        â†’ post-edit.sh          Format/lint after edits     â”‚
â”‚  PostToolUseFailure â†’ error-recovery.py     Classify & recover errors   â”‚
â”‚  SubagentStart      â†’ agent-tracker.py      Track parallel agents       â”‚
â”‚  SubagentStop       â†’ agent-synthesizer.py  Merge agent results         â”‚
â”‚  PreCompact         â†’ pre-compact.sh        Backup before compaction    â”‚
â”‚  Stop               â†’ dod-verifier.py       Verify Definition of Done   â”‚
â”‚  SessionEnd         â†’ session-end.sh        Cleanup and metrics         â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Loop Phases (Detailed)

### Phase 1: CONTEXTUALIZE (NEW in v3.0)
```
Load appropriate context for the task:

STEP 1: QUERY CLASSIFICATION (NEW in v3.2)
â”œâ”€â”€ Analyze task complexity and scope
â”œâ”€â”€ Classify as: depth-first, breadth-first, or straightforward
â”‚   â”œâ”€â”€ Depth-first: Complex problem needing deep analysis (1 agent, opus)
â”‚   â”œâ”€â”€ Breadth-first: Many independent subtasks (3-20 agents, haiku/sonnet)
â”‚   â””â”€â”€ Straightforward: Simple task, execute directly (no subagents)
â”œâ”€â”€ Determine subagent count based on complexity:
â”‚   â”œâ”€â”€ Simple (1-2 files): 1 agent or none
â”‚   â”œâ”€â”€ Medium (3-10 files): 3-5 agents
â”‚   â””â”€â”€ Complex (10+ files): 5-20 agents
â””â”€â”€ Set orchestration mode (standard/swarm/pipeline)

STEP 2: CONTEXT LOADING
â”œâ”€â”€ context-injector.py analyzes prompt keywords
â”œâ”€â”€ Load relevant files via smart-context-v3
â”œâ”€â”€ Load rules based on task type (task-orchestrator)
â”œâ”€â”€ Check context budget before heavy operations
â””â”€â”€ Initialize LOOP_STATE.md

STEP 3: AGENT SPAWNING (if breadth-first)
â”œâ”€â”€ Spawn parallel agents based on classification
â””â”€â”€ agent-tracker.py monitors parallel execution

Hooks Involved:
â”œâ”€â”€ UserPromptSubmit â†’ context-injector.py
â”œâ”€â”€ SubagentStart â†’ agent-tracker.py (if parallel)
â””â”€â”€ smart-context-v3 skill activated

Skills Involved:
â”œâ”€â”€ queue-manager (query classification)
â””â”€â”€ smart-context (context loading)
```

### Phase 2: ASSESS
```
Run /assess to understand current state:
- Codebase health score
- Existing issues by severity
- Test coverage gaps
- Security vulnerabilities
- Tech debt items
- Outdated dependencies
- Agent findings (if parallel agents used)

Hooks Involved:
â””â”€â”€ SubagentStop â†’ agent-synthesizer.py (merges findings)
```

### Phase 2.5: META-COGNITION (Intelligence Layer)
```
Consult capability inventory and make intelligent decisions:

STEP 1: LOAD CAPABILITY INVENTORY
â”œâ”€â”€ Read context/CAPABILITY_INVENTORY.md
â”œâ”€â”€ Know all 35 commands available
â”œâ”€â”€ Know all 15 agents available
â”œâ”€â”€ Know all 13 rules available
â””â”€â”€ Know decision matrix

STEP 2: EVALUATE SITUATION
â”œâ”€â”€ What type of work is this? (feature/bug/security/etc.)
â”œâ”€â”€ What's the current state? (coverage, issues, blockers)
â”œâ”€â”€ What's been tried before? (check LOOP_STATE.md)
â”œâ”€â”€ What's working? What's not?
â””â”€â”€ Are we making progress?

STEP 3: SELECT STRATEGY
â”œâ”€â”€ Consult decision matrix (situation â†’ best tool)
â”œâ”€â”€ Choose primary approach
â”œâ”€â”€ Identify backup approaches
â”œâ”€â”€ Determine if specialists needed
â”œâ”€â”€ Load relevant rules (@rules/*)
â””â”€â”€ Document decision rationale

STEP 4: PREPARE EXECUTION
â”œâ”€â”€ Queue specialists agents to spawn
â”œâ”€â”€ Load rules for context
â”œâ”€â”€ Set success metrics
â””â”€â”€ Define pivot triggers

Example Decisions:
â”œâ”€â”€ Coverage at 40% â†’ Spawn test-engineer agent
â”œâ”€â”€ Security issue found â†’ Spawn security-analyst, load @rules/security
â”œâ”€â”€ UI work needed â†’ Spawn ui-ux-expert, load @rules/ui-ux-design
â”œâ”€â”€ Stuck after 2 iterations â†’ Spawn researcher, try different approach
â””â”€â”€ Performance problem â†’ Spawn performance-optimizer, load @rules/performance
```

### Phase 3: PLAN (Enhanced with Decomposition)
```
If no active plan exists:
- Check for existing blueprint
- If features needed, ask user for direction (CHECKPOINT)
- Create prioritized work list

PARALLEL TASK DECOMPOSITION (NEW):
â”œâ”€â”€ Analyze work queue for independent tasks
â”œâ”€â”€ Identify parallelizable work units
â”œâ”€â”€ Group by: module, concern, layer, or type
â”œâ”€â”€ Assign appropriate agents to each unit
â”œâ”€â”€ Mark dependencies between units
â””â”€â”€ Execute independent units in parallel

Priority order:
1. S0 Critical issues (security, crashes)
2. S1 High issues (major bugs, blockers)
3. Test coverage gaps (< 80%)
4. S2 Medium issues
5. Tech debt items
6. S3 Low issues
7. Modernization opportunities

Skills Involved:
â”œâ”€â”€ parallel-task-decomposer (identifies parallel units)
â””â”€â”€ context-budget-monitor (checks if spawning needed)
```

### Phase 4: BUILD (Enhanced with Decision Logging)
```
For each item in priority order:
- Implement fix/feature
- Follow existing patterns (use @patterns/* when applicable)
- Use modern tech (check with context7)
- Use code snippets (snippet:* for boilerplate)
- Create/update tests
- Document changes

DECISION LOGGING (NEW):
â”œâ”€â”€ After each significant choice, log to DECISIONS_LOG.md
â”œâ”€â”€ Capture: decision, alternatives, rationale
â”œâ”€â”€ Link decisions to code locations
â”œâ”€â”€ Reference decisions in commit messages
â””â”€â”€ Categories: TECH, ARCH, TRADE, SCOPE, SEC, PERF, FIX, DEFER

CONTEXT MONITORING (NEW):
â”œâ”€â”€ Check context budget before heavy operations
â”œâ”€â”€ If > 70% utilized â†’ spawn sub-agent
â”œâ”€â”€ Delegate self-contained tasks to agents
â”œâ”€â”€ Keep main context lean and focused

Hooks Involved:
â”œâ”€â”€ PreToolUse â†’ file-validator.py (validate writes)
â”œâ”€â”€ PostToolUse â†’ post-edit.sh (format/lint)
â””â”€â”€ PostToolUseFailure â†’ error-recovery.py (handle errors)

Skills Involved:
â”œâ”€â”€ decision-logger (captures decisions)
â”œâ”€â”€ context-budget-monitor (prevents bloat)
â””â”€â”€ pattern/snippet surfacing (from task-orchestrator)
```

### Phase 5: TEST (Thresholds Enforced)
```
Run comprehensive testing with STRICT thresholds:
- Unit tests: ALL must pass (0 failures allowed)
- Coverage: >= 80% overall, >= 90% for new code
- Integration tests: ALL must pass
- Type checking: 0 errors allowed
- Lint checks: 0 errors, 0 warnings
- Build verification: Must succeed
- Browser verification (if UI changes)

Quality Gates Checked (ALL BLOCKING):
â”œâ”€â”€ GATE 2: LINT (0 errors, 0 warnings)
â”œâ”€â”€ GATE 3: TYPE (0 errors)
â”œâ”€â”€ GATE 4: UNIT TEST (100% pass, 80% coverage)
â”œâ”€â”€ GATE 5: INTEGRATION (100% pass)
â””â”€â”€ GATE 8: BROWSER (if applicable)

If ANY gate fails, add to work queue and continue loop.
```

### Phase 6: QUALITY (ALL GATES BLOCKING)
```
Run all quality gates â€” EVERY GATE IS BLOCKING:

CODE QUALITY GATES:
GATE 1:  â–¡ PRE-FLIGHT      Environment, dependencies, git state
GATE 2:  â–¡ LINT            0 errors, 0 warnings (BLOCKING)
GATE 3:  â–¡ TYPE            0 type errors (BLOCKING)
GATE 4:  â–¡ UNIT TEST       100% pass, coverage >= 80% (BLOCKING)
GATE 5:  â–¡ INTEGRATION     100% pass (BLOCKING)
GATE 6:  â–¡ SECURITY        0 S0/S1 vulnerabilities (BLOCKING)
GATE 7:  â–¡ PERFORMANCE     Within limits (BLOCKING)
GATE 8:  â–¡ BROWSER         Pass visual check (BLOCKING if UI)
GATE 9:  â–¡ ACCESSIBILITY   Pass a11y audit (BLOCKING if UI)
GATE 10: â–¡ DOCUMENTATION   README + CHANGELOG present (BLOCKING)
GATE 11: â–¡ MODERN TECH     No deprecated APIs (BLOCKING)

WORK COMPLETION GATES:
GATE 12: â–¡ WORK QUEUE      0 pending, 0 in-progress tasks (BLOCKING)
GATE 13: â–¡ KNOWN ISSUES    0 S0/S1 unresolved (BLOCKING)
GATE 14: â–¡ GIT STATE       All changes committed (BLOCKING)
GATE 15: â–¡ DEFINITION OF DONE  dod-verifier.py exit code 0 (BLOCKING)

If ANY gate fails â†’ Trigger evaluator-optimizer loop:
â”œâ”€â”€ EVALUATE: Identify which gate(s) failed and why
â”œâ”€â”€ OPTIMIZE: Generate fix for the failing condition
â”œâ”€â”€ RE-EVALUATE: Run gate again after fix
â”œâ”€â”€ ITERATE: Continue until gate passes or max iterations (3)
â””â”€â”€ If still failing after 3 iterations â†’ Add to work queue, continue loop

Skills Involved:
â”œâ”€â”€ evaluator-optimizer (quality gate retry loops)
â””â”€â”€ definition-of-done (completion criteria)
```

### Phase 6.25: CHECKPOINT (Commit Verified Work)
```
After quality gates pass for a feature, create a checkpoint commit:

CHECKPOINT CRITERIA (all must be true):
â”œâ”€â”€ Tests pass for the feature
â”œâ”€â”€ Linting passes (no errors, no warnings)
â”œâ”€â”€ Type checking passes
â”œâ”€â”€ Feature works as intended
â”œâ”€â”€ No debug code left behind
â””â”€â”€ Changes are logically complete

CHECKPOINT PROCESS:
â”œâ”€â”€ Stage relevant files (not all files)
â”œâ”€â”€ Create descriptive commit message
â”œâ”€â”€ Include decision references (Decisions: TECH-001, etc.)
â”œâ”€â”€ Update CHECKPOINTS.md with checkpoint info
â”œâ”€â”€ Tag milestone if significant
â””â”€â”€ Update LOOP_STATE.md with last checkpoint

COMMIT MESSAGE FORMAT:
â”œâ”€â”€ Type: feat/fix/refactor/test/docs
â”œâ”€â”€ Scope: (module)
â”œâ”€â”€ Description: what was done
â”œâ”€â”€ Details: bullet points
â”œâ”€â”€ Decisions: references to DECISIONS_LOG.md
â””â”€â”€ Co-Author tag

ROLLBACK SAFETY:
â”œâ”€â”€ Each checkpoint is a potential rollback point
â”œâ”€â”€ If next feature breaks something â†’ revert to checkpoint
â”œâ”€â”€ Clean history enables git bisect
â””â”€â”€ Easy to identify which change caused issues

Skills Involved:
â””â”€â”€ commit-checkpoint (creates verified commits)
```

### Phase 6.5: REASSESS (Pivot Check)
```
Check if current approach is working, pivot if needed:

TRIGGER CONDITIONS (any of these = reassess):
â”œâ”€â”€ Same gate failed 3+ times
â”œâ”€â”€ No improvement for 2 iterations
â”œâ”€â”€ Coverage not increasing
â”œâ”€â”€ Issues count not decreasing
â”œâ”€â”€ Error recovery failed
â””â”€â”€ User requested reassessment

REASSESSMENT PROCESS:

STEP 1: ANALYZE CURRENT STATE
â”œâ”€â”€ What was the goal?
â”œâ”€â”€ What approach was taken?
â”œâ”€â”€ What's the actual result?
â”œâ”€â”€ Where are we stuck?
â””â”€â”€ Why isn't it working?

STEP 2: CONSULT CAPABILITY INVENTORY
â”œâ”€â”€ What other tools could address this?
â”œâ”€â”€ Is there a specialist agent for this problem?
â”œâ”€â”€ Is there a rule with specific guidance?
â”œâ”€â”€ Has context7 docs on this topic?
â””â”€â”€ Can we break the problem down differently?

STEP 3: SELECT NEW STRATEGY
â”œâ”€â”€ Choose alternative from decision matrix
â”œâ”€â”€ Spawn specialist agent if not already tried
â”œâ”€â”€ Load additional rules for guidance
â”œâ”€â”€ Consider simplifying the problem
â”œâ”€â”€ If truly stuck â†’ Ask user (last resort)

STEP 4: LOG AND PIVOT
â”œâ”€â”€ Document current approach result in LOOP_STATE.md
â”œâ”€â”€ Document new strategy selection
â”œâ”€â”€ Reset stuck counter
â”œâ”€â”€ Update work queue with new approach
â””â”€â”€ Return to CONTEXTUALIZE with fresh strategy

EXAMPLE PIVOTS:
â”œâ”€â”€ Coverage stuck at 50% after 3 tries
â”‚   â†’ Spawn test-engineer agent
â”‚   â†’ "Generate comprehensive test suite for uncovered modules"
â”‚
â”œâ”€â”€ Lint errors keep reappearing
â”‚   â†’ Load @rules/code-quality
â”‚   â†’ Spawn code-reviewer for deeper analysis
â”‚
â”œâ”€â”€ Security issue won't resolve
â”‚   â†’ Spawn security-analyst (opus model)
â”‚   â†’ Load @rules/security for OWASP guidance
â”‚
â””â”€â”€ Build keeps failing
    â†’ Spawn researcher to investigate root cause
    â†’ Check context7 for framework-specific docs
```

### Phase 7: EVALUATE (STRICT ENFORCEMENT)
```
Check completion criteria with dod-verifier.py:

ALL MUST BE TRUE â€” NO EXCEPTIONS:
â”œâ”€â”€ Coverage >= 80% overall (VERIFIED by running tests)
â”œâ”€â”€ Coverage >= 90% for new code
â”œâ”€â”€ Lint errors = 0 (VERIFIED by running linter)
â”œâ”€â”€ Lint warnings = 0
â”œâ”€â”€ Type errors = 0 (VERIFIED by running type checker)
â”œâ”€â”€ S0 vulnerabilities = 0 (VERIFIED by security scan)
â”œâ”€â”€ S1 vulnerabilities = 0
â”œâ”€â”€ Failing tests = 0 (VERIFIED by running tests)
â”œâ”€â”€ README.md exists and current
â”œâ”€â”€ CHANGELOG.md updated
â”œâ”€â”€ All 12 quality gates passing
â”œâ”€â”€ Work queue empty (no pending items)
â””â”€â”€ 2 consecutive passing iterations

IF ANY THRESHOLD NOT MET:
â”œâ”€â”€ dod-verifier.py returns exit code 1
â”œâ”€â”€ Add missing items to work queue
â”œâ”€â”€ Go back to Phase 1 (CONTEXTUALIZE)
â”œâ”€â”€ Loop continues â€” NO EXIT ALLOWED

Hooks Involved:
â””â”€â”€ Stop â†’ dod-verifier.py (exit code 0 = pass, 1 = fail)
```

### Phase 8: RECOVER (NEW in v3.0)
```
Handle errors and recover gracefully:
- Classify error type (error-classifier skill)
- Apply recovery strategy:
  â”œâ”€â”€ Transient: Retry with backoff
  â”œâ”€â”€ Actionable: Add fix to queue
  â”œâ”€â”€ Blocking: Pause for user
- Update work queue with discovered issues
- Return to Phase 1 (CONTEXTUALIZE)

Hooks Involved:
â””â”€â”€ PostToolUseFailure â†’ error-recovery.py

Recovery Actions:
â”œâ”€â”€ Network errors â†’ Retry with exponential backoff
â”œâ”€â”€ Module not found â†’ Suggest npm install
â”œâ”€â”€ Type errors â†’ Add to queue as S1
â”œâ”€â”€ Syntax errors â†’ Add to queue as S0 (priority)
â”œâ”€â”€ Permission errors â†’ Pause for user intervention
â””â”€â”€ Rate limits â†’ Wait and retry
```

## Dynamic Work Queue

**The loop continues when new work is discovered:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  WORK QUEUE MANAGEMENT                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  During any phase, if new issues/features are discovered:  â”‚
â”‚                                                             â”‚
â”‚  1. Add to work queue with appropriate priority            â”‚
â”‚  2. Continue current iteration                              â”‚
â”‚  3. Process new items in subsequent iterations             â”‚
â”‚                                                             â”‚
â”‚  Examples:                                                  â”‚
â”‚  - Fixing bug A reveals bug B â†’ Add B to queue             â”‚
â”‚  - Security fix needs new tests â†’ Add tests to queue       â”‚
â”‚  - Modernization reveals deprecated API â†’ Add to queue     â”‚
â”‚  - User requests feature mid-loop â†’ Add to queue           â”‚
â”‚                                                             â”‚
â”‚  The loop ONLY exits when:                                  â”‚
â”‚  âœ… Work queue is completely empty                          â”‚
â”‚  âœ… All quality gates pass                                  â”‚
â”‚  âœ… No new issues discovered in final iteration            â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Work Queue Priority

New items are inserted by priority:
```
Priority Order:
1. S0 Critical (security, crashes) â†’ Top of queue
2. S1 High (major bugs, blockers) â†’ After S0
3. Blocking dependencies â†’ After S1
4. S2 Medium â†’ Middle
5. Test coverage gaps â†’ Middle
6. S3 Low â†’ Bottom
7. Enhancements/modernization â†’ Bottom

Example queue evolution:

Iteration 1:
  [S1] Fix auth bypass
  [S2] Add input validation
  [S3] Refactor utils

Iteration 2 (discovered new issue while fixing auth):
  [S1] Fix session handling â† NEW (discovered)
  [S2] Add input validation
  [S2] Add tests for auth fix â† NEW (required)
  [S3] Refactor utils

Iteration 3 (user added feature request):
  [S2] Add input validation
  [S2] Add tests for auth fix
  [S2] Implement caching â† NEW (user request)
  [S3] Refactor utils
  [S3] Modernize date handling â† NEW (discovered)
```

### Accepting New Work Mid-Loop

The loop accepts new work from:

1. **Discovered Issues**
   - Bugs found while fixing other bugs
   - Security issues revealed by fixes
   - Test gaps identified during coverage push

2. **User Requests**
   - "Also add rate limiting"
   - "Include this feature too"
   - New requirements mid-session

3. **Modernization Discoveries**
   - Deprecated APIs found during review
   - Outdated patterns identified
   - Vulnerable dependencies discovered

4. **Quality Gate Failures**
   - Test failures create fix tasks
   - Review findings become work items
   - Security scan results added to queue

**The loop continues until the queue is empty AND all gates pass.**

## State Tracking

Create/update `LOOP_STATE.md`:

```markdown
# Autonomous Loop State

## Current Iteration: 7
## Started: 2024-01-15T10:00:00Z
## Status: IN_PROGRESS

## Quality Gates
| Gate | Status | Last Check | Issues |
|------|--------|------------|--------|
| Tests | âœ… PASS | 10:45:00 | 0 |
| Coverage | âŒ FAIL | 10:45:00 | 78% (need 80%) |
| Security | âœ… PASS | 10:42:00 | 0 S0/S1 |
| Lint | âœ… PASS | 10:40:00 | 0 |
| Work Queue | âŒ FAIL | 10:45:00 | 2 pending |
| Documentation | âœ… PASS | 10:35:00 | Complete |

## Work Queue
| Priority | Item | Status |
|----------|------|--------|
| S1 | Fix auth bypass | âœ… Done |
| S2 | Add input validation | ğŸ”„ In Progress |
| S2 | Increase test coverage | â³ Pending |
| S3 | Refactor utils | â³ Pending |

## Approaches Tried (Meta-Cognition)
| Iteration | Approach | Result | Next Action |
|-----------|----------|--------|-------------|
| 1-3 | Direct implementation | Stuck at 40% coverage | Pivot to specialist |
| 4-5 | Spawned test-engineer | Coverage 40%â†’65% | Continue |
| 6-7 | Continue with test-engineer | Coverage 65%â†’78% | Continue |

## Pivots Made
- **Iteration 4**: Pivoted from direct implementation to test-engineer agent
  - Reason: Coverage stuck at 40% for 3 iterations
  - Result: Immediate improvement

## Agents Spawned
| Agent | Iteration | Purpose | Outcome |
|-------|-----------|---------|---------|
| test-engineer | 4 | Generate tests for coverage | +38% coverage |
| security-analyst | 5 | Audit auth module | Found 1 S1 issue |

## Iteration History
- #7: Coverage 75%â†’78%, added 3 more tests
- #6: Coverage 65%â†’75%, test-engineer added 8 tests
- #5: Fixed S1 security issue found by security-analyst
- #4: Pivoted to test-engineer agent
- #3: Stuck - coverage still 40%
- #2: Coverage still 40%, attempted direct test writing
- #1: Initial assessment - coverage 40%

## Stuck Counter
- Current streak without improvement: 0
- Trigger reassessment at: 2 iterations

## Checkpoints (User Input Required)
- [ ] Iteration 3: Confirmed feature direction
- [ ] Iteration 5: Approved breaking change
```

## User Checkpoints

The loop PAUSES for user input when:

1. **Direction Needed**
   - No clear next feature to implement
   - Multiple valid approaches exist
   - Breaking change required

2. **S0 Critical Decision**
   - Security issue with multiple fix strategies
   - Data migration required
   - External service decision

3. **Resource Question**
   - New dependency needed
   - Infrastructure change required
   - Cost implications

4. **Explicit Request**
   - User previously said "ask before X"

## Modern Tech Awareness

Each iteration checks:

```
1. Use context7 to verify current patterns
2. Check if dependencies have newer major versions
3. Identify deprecated APIs in use
4. Suggest modern alternatives when beneficial

Example triggers:
- Using moment.js â†’ Suggest date-fns or dayjs
- Using request â†’ Suggest fetch or axios
- Using callback patterns â†’ Suggest async/await
- Using var â†’ Suggest const/let
```

## Exit Conditions (STRICT ENFORCEMENT)

### Success Exit â€” ALL THRESHOLDS MUST BE MET
```
The loop CANNOT exit until ALL of the following are true:

MANDATORY THRESHOLDS (non-negotiable):
â”œâ”€â”€ Test coverage >= 80% overall
â”œâ”€â”€ Test coverage >= 90% for new/changed code
â”œâ”€â”€ 0 lint errors
â”œâ”€â”€ 0 lint warnings
â”œâ”€â”€ 0 type errors
â”œâ”€â”€ 0 S0 (critical) security vulnerabilities
â”œâ”€â”€ 0 S1 (high) security vulnerabilities
â”œâ”€â”€ 0 failing tests
â”œâ”€â”€ Documentation complete (README + CHANGELOG)
â”œâ”€â”€ All 12 quality gates passing
â”œâ”€â”€ 2 consecutive passing iterations
â”œâ”€â”€ Verification iteration confirms stability

VERIFICATION PROCESS:
â”œâ”€â”€ After 2 consecutive passes, run ONE MORE full check
â”œâ”€â”€ Re-run all 12 quality gates with dod-verifier.py
â”œâ”€â”€ dod-verifier.py actually runs tests/lint/coverage
â”œâ”€â”€ If exit code 0 (all pass) â†’ EXIT SUCCESS
â”œâ”€â”€ If exit code 1 (any fail) â†’ Back to Phase 1 (reset counter)

Hook: Stop â†’ dod-verifier.py returns exit code 0 or 1
```

### NO EXCEPTIONS
```
There are NO warnings. There are NO "continue anyway" options.
Every threshold must be met. Period.

If a threshold cannot be met:
â”œâ”€â”€ Add work items to fix the issue
â”œâ”€â”€ Loop continues
â”œâ”€â”€ User can adjust thresholds in dod-verifier.py THRESHOLDS dict
â”œâ”€â”€ But default thresholds are the standard
```

### Forced Exit
```
--max-iterations reached
OR user types "stop loop" / "pause"
OR unrecoverable error (RECOVER phase escalates)
OR loop detects no progress (same issues 3 iterations)

Session cleanup:
Hook: SessionEnd â†’ session-end.sh
â”œâ”€â”€ Save metrics to .claude/metrics/
â”œâ”€â”€ Update SESSION_HISTORY.md
â”œâ”€â”€ Clean up temporary state
â””â”€â”€ Report session summary
```

## Output

Each iteration produces:
```markdown
## Loop Iteration #N Summary

### Quality Gate Status
[Table of all gates with pass/fail]

### Work Completed
- [List of items fixed/implemented]

### Remaining Issues
- [Prioritized list]

### Next Iteration Plan
- [What will be attempted next]

### Time Elapsed: Xm Ys
### Estimated Remaining: ~N iterations
```

## Final Report

When loop completes:
```markdown
# Autonomous Loop Complete

## Summary
- Total iterations: 12
- Total time: 45 minutes
- Issues fixed: 23
- Tests added: 15
- Coverage: 72% â†’ 91%

## Quality Gates - ALL PASSING
âœ… Tests: 156 passing
âœ… Coverage: 91%
âœ… Security: 0 vulnerabilities
âœ… Code Quality: A rating
âœ… Documentation: Complete

## Changes Made
[Detailed list with file references]

## Recommendations
[Any remaining S3/optional improvements]
```

## Safety Guardrails

1. **No destructive operations without confirmation**
2. **Git commit after each successful iteration**
3. **Rollback capability if iteration fails**
4. **Max iterations prevent infinite loops**
5. **Stall detection (no progress = pause)**
6. **Resource limits respected**

---

## Integrated Skills

The autonomous loop coordinates with these skills:

| Skill | Role in Loop |
|-------|--------------|
| `task-orchestrator` | Classifies task, loads rules/agents at start |
| `definition-of-done` | Provides completion criteria in Phase 6 |
| `queue-manager` | Manages and visualizes work queue |
| `result-synthesizer` | Combines multi-agent findings |
| `browser-verification` | Verifies UI changes in Phase 4/5 |
| `smart-context` | Optimizes context loading |
| `session-memory` | Prevents redundant file reads |
| `context-budget-monitor` | Prevents context bloat, suggests sub-agents |
| `parallel-task-decomposer` | Breaks tasks into parallel work units |
| `decision-logger` | Captures decisions for traceability |
| `commit-checkpoint` | Creates verified commits for rollback |
| `evaluator-optimizer` | Feedback loops for quality gate failures |
| `queue-manager` | Query classification and work queue |

### Loop Startup Sequence

```
/loop "implement user authentication"
      â”‚
      â–¼
1. task-orchestrator analyzes task
   â”œâ”€â”€ Classifies: Security-Critical Feature
   â”œâ”€â”€ Loads: @rules/security, @rules/api-design, @rules/testing
   â””â”€â”€ Suggests: security-analyst agent
      â”‚
      â–¼
2. Spawn recommended agents (if applicable)
   â””â”€â”€ security-analyst reviews codebase
      â”‚
      â–¼
3. result-synthesizer combines findings
   â””â”€â”€ Creates unified work queue
      â”‚
      â–¼
4. queue-manager initializes LOOP_STATE.md
      â”‚
      â–¼
5. Begin Phase 1: ASSESS
```

### Browser Verification Integration

For UI-related tasks:

```
Phase 4: TEST (extended for UI)
â”œâ”€â”€ Run unit tests
â”œâ”€â”€ Run integration tests
â””â”€â”€ IF UI changes detected:
    â””â”€â”€ browser-verification activates
        â”œâ”€â”€ Screenshot at viewports
        â”œâ”€â”€ Accessibility audit
        â”œâ”€â”€ Form testing (if forms)
        â””â”€â”€ Report visual issues

Phase 5: QUALITY (extended for UI)
â”œâ”€â”€ Standard quality gates
â””â”€â”€ IF UI changes:
    â”œâ”€â”€ Visual regression check
    â””â”€â”€ Add visual issues to queue
```

### Context Optimization

To prevent context exhaustion during long loops:

```
Every 5 iterations:
â”œâ”€â”€ Summarize progress to LOOP_STATE.md
â”œâ”€â”€ Clear completed item details from context
â”œâ”€â”€ Keep only active work item context
â””â”€â”€ Reference summaries instead of full content

When context > 70% utilized:
â”œâ”€â”€ Offload analysis to forked agent
â”œâ”€â”€ Agent returns summary only
â””â”€â”€ Main context stays lean

For complex analysis:
â”œâ”€â”€ Spawn specialized agent
â”œâ”€â”€ Agent explores in isolation
â”œâ”€â”€ Returns structured findings
â””â”€â”€ Main loop continues with findings
```

---

## Reference

See `_system.md` for complete platform architecture and component relationships.
